from abc import ABCMeta
import gym
import gym_cenvs
import numpy as np
import logging
from src.simp_mod_library.simp_mod_lib import SimpModLib
import torch
from typing import List


class BaseAgent(metaclass=ABCMeta):
    """
    Base class for constructing agents that control the complex object using passed Simple Model Library
    """
    def __init__(self, device: str = 'cuda:0'):
        # String that is set in derived class
        self.env_name: str = None
        # Ref to step-able mujoco-gym environment
        self.env = None

        # Devices
        self.device = device

        # Dimensionality of an action vector for env
        self.action_dimension: int = None
        # Number of times we repeat the same action (action repetition, task specific)
        self.actions_per_loop: int = None
        # Dimensionality of ground truth state for an env
        self.gt_state_dim: int = None
        # Episode horizon set for task
        self.episode_T: int = None

        self.model_lib: SimpModLib = None

        # Keys for the online collected dataset. gt = Ground Truth, ep = episode,
        # gt_frame = raw unprocessed frame
        # all_ep_rollouts = potentially hybrid multiple model roll-outs generated by
        #  propagating the final planned actions
        self.data_keys = ["action",
                          "gt_state",
                          "gt_frame"
                          ]

        # Container for the episode data collected that is global across agent
        self.episode_history = dict()
        # Initialize all the episode-specific datasets with empty lists
        for data_key in self.data_keys:
            self.episode_history[data_key] = []

        # Vars to track agent specific objects
        self.gt_state: torch.Tensor = None

        # Special indices within gt_state that are passed down to SML
        self.rob_gt_idx: List[int] = None

        # - - - - - - - - - - - - - - - - - - - -
        # TODO: Keep these exclusively in task specific Agents
        # Parameter priors that are part of agent configuration
        self.rob_mass: float = 1.0
        # - - - - - - - - - - - - - - - - - - - -

        self.dir_manager = None

        # Simple Model visualization related
        # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
        # TODO: Change visulization method to be a single function invoked by agent
        #  so make a library level viz object a part of the agent definition
        # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

    def make_agent_for_task(self, smodel_list: List[str]):
        """
        Invoked after task specific params have been set in derived class
        :return:
        """
        self.env = gym.make(self.env_name)
        self.env.seed(0)
        self.env.action_space.seed(0)

        # TODO: Check consistency of action dimension with simple model lib here

        self.gt_state = torch.zeros(self.gt_state_dim, dtype=torch.float32)

        # Initialize SML
        self.model_lib = SimpModLib(model_names=smodel_list, dir_manager=self.dir_manager, rob_mass=self.rob_mass,
                                    device=self.device)

        return

    @classmethod
    def __new__(cls, *args, **kwargs):
        """
        Make abstract base class non-instaiable
        :param args:
        :param kwargs:
        """
        if cls is BaseAgent:
            raise TypeError(f"only children of '{cls.__name__}' may be instantiated")
        return object.__new__(cls)

    def do_episode(self):
        """
        Agent method to online interact with the complex env over an episode
         While doing an episode, there are no parameter updates, only the datasets are appended
        :return:
        """
        # no grad ensures no parameter updates (also less memory since no grads are tracked)
        with torch.no_grad():
            while True:
                # Clear episode specific state
                self.reset_episode()
                # Reward accumulated over course of episode
                cum_reward = 0.0
                for t in range(0, self.episode_T, self.actions_per_loop):
                    done, fail, reward, info = self.step()
                    cum_reward += reward

                    if done or fail:
                        break
                # Reject episodes shorter than
                if t < 5:
                    continue
                break
            fail = not info['success']

        return fail, t

    def step(self):
        """
        Step through the environment with a task specific agent
        :return:
        """
        # TODO: Change random actions to planned actions from controller
        # Random action for testing
        action = np.random.uniform(-1, 1)
        actions = torch.tensor([[action]], device=self.device)

        # Total reward seen so far
        total_reward = 0

        # TODO: Loop over possibly longer sequence of actions instead of single action

        # Take the planned action in world and get observation
        obs, rew, done, info = self.env.step(action)

        # Extract ground truth state
        gt_state = info['state']

        total_reward += rew

        # Update the robot's GT state
        self.model_lib.rob_state = torch.from_numpy(gt_state[self.rob_gt_idx]).to(self.device)

        # Invoke predict method of underlying simple models
        self.model_lib.predict(actions)
        # Perform update step for the tracking of all simple models
        self.model_lib.update(obs)

        # Log data from step
        self.episode_history['action'].append(actions.cpu().detach().numpy())
        self.episode_history['gt_frame'].append(obs)
        self.episode_history['gt_state'].append(gt_state)
        # TODO: Add back roll-out caching after adding MPC planning
        # self.episode_history['rollout'].append(rollout)

        return done, False, total_reward, info

    def reset_episode(self):
        """
        Reset a single episode of interaction with the environment to move on to the next episode
         within the same trial
        :return:
        """
        if self.env is None:
            raise ValueError("Environment has not been made ... cannot reset episode...")

        for data_key in self.data_keys:
            # Use lists to account for variable traj length
            self.episode_history[data_key] = []

        # Frame returned initially from mjco-py environment has a jump from subsequent so ignore it
        _ = self.env.reset()
        obs, rew, done, info = self.env.step(np.zeros(self.action_dimension))
        # Fetch gt_state from info
        gt_state = info['state']

        # Reset the episode-specific params of the simple model library
        #  Particularly reinit state, also uses obs to initialize simple-model state estimates for planning
        self.model_lib.reset_episode(obs)

        # Append GT state of complex environment to online collected data
        self.episode_history['gt_state'].append(gt_state)

        logging.info("Reset episode done for {0} agent".format(self.env_name.capitalize()))

    def reset_trial(self):
        """
        Reset an entire trial for a clean/fresh evaluation of MM-LVSPC
        :return:
        """
        self.model_lib.reset_trial()

    def save_episode_data(self):
        """
        Assemble all the fields to be saved into a single non-nested dictionary
        and save as a npz file
        :return:
        """
        # Ensured during construction that all the data appended to the online
        # collected lists are numpy arrays
        full_ep_history = {}

        for key in self.data_keys:
            full_ep_history[key] = np.array(self.episode_history[key])

        # Invoke the save methods of library and the books within library
        #  and get back the dictionaries associated with them
        #  modify their key by pre-pending corresponding smodel name
        for smodel in self.model_lib.model_names:
            # Get the dict corresponding to smodel
            smodel_ep_data = self.model_lib[smodel].episode_history
            for key in self.model_lib[smodel].data_keys:
                new_key = smodel + "_" + key
                full_ep_history[new_key] = smodel_ep_data[key]

        npz_file_path = self.dir_manager.get_abs_path('run_log_data') + '/tmp.npz'
        np.savez(npz_file_path, **full_ep_history)
